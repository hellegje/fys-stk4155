{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression, from linear and logistic regression to neural networks\n",
    "\n",
    "## Regression:\n",
    "Bring back regression code from project 1 (reference own work in bibliography) and compare to results from FFNN\n",
    "Data can be:\n",
    "- simple 1d\n",
    "- then probably test with Franke before reading image data from cancer data set\n",
    "\n",
    "\n",
    "## Classification:\n",
    "Develop a logistic regression code and compare to FFNN.\n",
    "Proposed data set:\n",
    "- Wisconsin Breast Cancer Data (images representing various tumor features)\n",
    "- Andre data (men pass pÃ¥ at kan diskutere resultatene mot literature)\n",
    "\n",
    "## Part c) Testing different activation functions for the hidden layers\n",
    "- Sigmoid\n",
    "- RELU\n",
    "- Leaky RELU\n",
    "Discuss the results\n",
    "May also study the way you initialise the  weights and biases\n",
    "\n",
    "## Part d) Classification analysis using neural networks\n",
    "\n",
    "## Part e) Write your logistic regression code\n",
    "To compare the FFNN code with logistic regression\n",
    "\n",
    "## Part f) Critical evaluation of the various algorithms\n",
    "Compare the results from the different algorithms, which works best for the regression case, and which one for classification? List pros and cons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a) Write own Stochastic Gradient Descent Code: First step\n",
    "Replace matrix inversion from OLS and Ridge with GD and SGD\n",
    "Data can be either\n",
    "- Franke\n",
    "- Terrain \n",
    "- Recommended: something simple like for instance: f(x) = ao + a1x + a2x^2\n",
    "look at exercises from week 41 for help\n",
    "\n",
    "## Analyse the GD and SGD code. This should include:\n",
    "(Lecture notes week 39 and 40 contain examples it is ok to use)\n",
    "### 1: plain gradient descent with fixed learning rate (tune it) using the analytical expression for the gradient\n",
    "### 2: Add momentum to the plain GD code and compare convergence with a fixed learning rate (may need to tune it). Keep using the analytical expression for the gradient\n",
    "### 3: Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. Use a tunable learning rate (lectures week 39 and 40). Discuss the results as functions of the various parameters (size of batches, nr of epochs etc). Use the analytical gradient\n",
    "### 4: Implement the adagrad method in order to tune the learning rate. Do this with and without momentum for plain gradient descent and SGD\n",
    "### 5: Add RMSprop and Adam to your library of methods for tuning the learning rate\n",
    "### 6: Replace thereafter your analytical gradient with either Autograd or JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "\n",
    "def MSE(expected_values, predicted_values):\n",
    "    n = np.size(predicted_values)  # Number of data points\n",
    "    return np.sum((expected_values - predicted_values)**2)/n\n",
    "\n",
    "class LinearRegressionModel(Enum):\n",
    "    OrdinaryLeastSquares = 0,\n",
    "    Ridge = 1\n",
    "\n",
    "class ScalingAlgorithm(Enum):\n",
    "    GradientDescent = 0,\n",
    "    MomentumGD = 1, #TODO: This is probably not relevant here anymore\n",
    "    StochasticGD = 2,\n",
    "    Adagrad = 3,\n",
    "    RMSprop = 4,\n",
    "    Adam = 5\n",
    "\n",
    "class OLS_Hessian:\n",
    "    def __init__(self, n, X):\n",
    "        self.H = (2.0/n)* X.T @ X\n",
    "        self.EigValues, self.EigVectors = np.linalg.eig(self.H)\n",
    "\n",
    "class Ridge_Hessian:\n",
    "    def __init__(self, n, lmbda, XT_X):\n",
    "        self.H = (2.0/n)* XT_X+2*lmbda* np.eye(XT_X.shape[0])\n",
    "        self.EigValues, self.EigVectors = np.linalg.eig(self.H)\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, x, y):\n",
    "        # Design matrix including the intercept\n",
    "        # No scaling of data and all data used for training \n",
    "        n = x.shape[0]\n",
    "        self.design_matrix = np.c_[np.ones((n, 1)), x, x*x]\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        self.ols_hessian = None\n",
    "        self.ridge_hessian = None\n",
    "    \n",
    "    def _grad(self, eta, gradient):\n",
    "        return eta*gradient\n",
    "\n",
    "    def _momentum_grad(self, eta, gradient, momentum, change):\n",
    "        return eta*gradient + momentum*change\n",
    "\n",
    "    def _adagrad(self, gradients, eta, G_iter):\n",
    "        delta  = 1e-8 # AdaGrad parameter to avoid possible division by zero\n",
    "\n",
    "        G_iter += gradients*gradients\n",
    "        update = gradients*eta/(delta+np.sqrt(G_iter))\n",
    "\n",
    "        return [update, G_iter]\n",
    "    \n",
    "    def gradient_descent_OLS(self, n, n_iterations, eta, scaling_algorithm = ScalingAlgorithm.GradientDescent, rho = None, momentum = None):\n",
    "        #Don't wanna calculate this every time when running multiple iterations\n",
    "        if not self.ols_hessian:\n",
    "            self.ols_hessian = OLS_Hessian(n, self.design_matrix)\n",
    "        \n",
    "        betas, scores = list(), list()\n",
    "        beta = np.random.randn(3,1)\n",
    "        #eta = eta/np.max(self.ols_hessian.EigValues)\n",
    "        change, G_iter = 0.0, np.zeros_like(beta)\n",
    "        delta  = 1e-8\n",
    "\n",
    "        for iter in range(n_iterations):\n",
    "            gradient = (2.0/n)*self.design_matrix.T @ (self.design_matrix @ beta-self.y)\n",
    "            match scaling_algorithm:\n",
    "                case ScalingAlgorithm.GradientDescent:\n",
    "                    beta -= self._grad(eta, gradient)\n",
    "\n",
    "                case ScalingAlgorithm.MomentumGD:\n",
    "                    new_change = self._momentum_grad(eta, gradient, momentum, change)\n",
    "                    change = new_change\n",
    "                    beta -= new_change\n",
    "\n",
    "                case ScalingAlgorithm.Adagrad:\n",
    "                    update, new_G_iter = self._adagrad(gradient, eta, G_iter)\n",
    "                    G_iter = new_G_iter\n",
    "                    if momentum:\n",
    "                        update = momentum*change + update\n",
    "                        change = update\n",
    "                    beta -= update\n",
    "\n",
    "                case ScalingAlgorithm.RMSprop:\n",
    "                    if rho is None:\n",
    "                        raise ValueError(\"Decay rate cannot be null when scaling with RMSprop\")\n",
    "                    \n",
    "                    G_iter = rho*G_iter + (1-rho) * gradient**2\n",
    "                    update = gradient*eta/(delta+np.sqrt(G_iter))\n",
    "                    if momentum:\n",
    "                        change = momentum*change - update\n",
    "                        beta += change\n",
    "                    else:\n",
    "                        beta -= update\n",
    "                \n",
    "                case _:\n",
    "                    raise NotImplementedError()\n",
    "                \n",
    "            #Store results\n",
    "            betas.append(beta)\n",
    "            scores.append(MSE(self.y, self.design_matrix @ beta))\n",
    "\n",
    "        return [betas, scores]\n",
    "    \n",
    "    #Function contents taken from lecture notes\n",
    "    def gradient_descent_ridge(self, n, n_iterations, eta, lmbda, momentum = None): \n",
    "        #Don't wanna calculate this every time when running multiple iterations\n",
    "        if not self.ridge_hessian:\n",
    "            XT_X = self.design_matrix.T @ self.design_matrix\n",
    "            self.ridge_hessian = Ridge_Hessian(n, lmbda, XT_X)\n",
    "        \n",
    "        #Id = n*lmbda* np.eye((XT_X).shape[0])\n",
    "\n",
    "        betas, scores = list(), list()\n",
    "        beta = np.random.randn(3,1)\n",
    "        #eta = eta/np.max(self.ridge_hessian.EigValues)\n",
    "        change = 0.0\n",
    "        for iter in range(n_iterations):\n",
    "            gradient = 2.0/n*self.design_matrix.T @ (self.design_matrix @ beta-self.y)+2*lmbda*beta\n",
    "            #Gradient descent with momentum:\n",
    "            if momentum:\n",
    "                new_change = eta*gradient + momentum*change\n",
    "                change = new_change\n",
    "                beta -= new_change\n",
    "\n",
    "                #Store results\n",
    "                betas.append(beta)\n",
    "                scores.append(MSE(self.y, self.design_matrix @ beta))\n",
    "            #Gradient descent without momentum:\n",
    "            else:\n",
    "                beta -= eta*gradient\n",
    "\n",
    "                #Store results\n",
    "                betas.append(beta)\n",
    "                scores.append(MSE(self.y, self.design_matrix @ beta))\n",
    "\n",
    "        return [betas, scores]\n",
    "    \n",
    "    def _learning_schedule(self, t):\n",
    "        t0, t1 = 5, 50\n",
    "        return t0/(t+t1)\n",
    "    \n",
    "    def _SGD(self, beta, lin_reg_model, epoch, M, m, scaling_algorithm = ScalingAlgorithm.StochasticGD, lmbda = None, rho = None, beta_values=None, momentum = None):\n",
    "        G_iter = np.zeros_like(beta)\n",
    "        change = np.zeros_like(beta) if momentum else None\n",
    "        delta = 1e-8 #Constant for AdaGrad\n",
    "        first_moment = np.zeros_like(beta)  # Initialize for Adam first moment\n",
    "        second_moment = np.zeros_like(beta)  # Initialize for Adam second moment\n",
    "\n",
    "        for iter in range(m):\n",
    "            random_index = M*np.random.randint(m)\n",
    "            xi = self.design_matrix[random_index:random_index+M]\n",
    "            yi = self.y[random_index:random_index+M]\n",
    "\n",
    "            y_tilde = xi @ beta\n",
    "            gradients = (2.0/M)* xi.T @ (y_tilde-yi)\n",
    "\n",
    "            match lin_reg_model:\n",
    "                case LinearRegressionModel.Ridge:\n",
    "                    if not lmbda:\n",
    "                        raise ValueError(\"Lambda cannot be null for Ridge regression\")\n",
    "                    gradients += 2*lmbda*beta\n",
    "            \n",
    "            t = epoch*m+iter\n",
    "            eta = self._learning_schedule(t)\n",
    "            \n",
    "            match scaling_algorithm:\n",
    "                case ScalingAlgorithm.StochasticGD:\n",
    "                    if momentum:\n",
    "                        change = momentum*change - eta*gradients\n",
    "                        beta += change\n",
    "                    else:\n",
    "                        beta -= eta*gradients\n",
    "\n",
    "                case ScalingAlgorithm.Adagrad:\n",
    "                    G_iter += gradients * gradients\n",
    "                    update = gradients*eta/(delta+np.sqrt(G_iter))\n",
    "                    if momentum:\n",
    "                        change = momentum*change - update\n",
    "                        beta += change\n",
    "                    else:\n",
    "                        beta -= update\n",
    "\n",
    "                case ScalingAlgorithm.RMSprop:\n",
    "                    if rho is None:\n",
    "                        raise ValueError(\"Decay rate cannot be null when scaling with RMSprop\")\n",
    "                    G_iter = rho*G_iter + (1-rho) * gradients**2\n",
    "                    update = gradients*eta/(delta+np.sqrt(G_iter))\n",
    "                    if momentum:\n",
    "                        change = momentum*change - update\n",
    "                        beta += change\n",
    "                    else:\n",
    "                        beta -= update\n",
    "                \n",
    "                case ScalingAlgorithm.Adam:\n",
    "                    if beta_values is None:\n",
    "                        raise ValueError(\"Beta values must be provided to run with ADAM\")\n",
    "                    \n",
    "                    #From ChatGPT:\n",
    "                    t += 1  # Increment time step\n",
    "                    first_moment = beta_values[0] * first_moment + (1 - beta_values[0]) * gradients  # Update first moment\n",
    "                    second_moment = beta_values[1] * second_moment + (1 - beta_values[1]) * gradients**2  # Update second moment\n",
    "\n",
    "                    # Bias correction\n",
    "                    first_term = first_moment / (1 - beta_values[0]**t)  # Corrected first moment\n",
    "                    second_term = second_moment / (1 - beta_values[1]**t)  # Corrected second moment\n",
    "\n",
    "                    update = eta * first_term / (np.sqrt(second_term) + delta)  # Update parameters\n",
    "\n",
    "                    beta -= update\n",
    "                    \n",
    "                case _:\n",
    "                    raise NotImplementedError()\n",
    "\n",
    "        return beta\n",
    "\n",
    "    \n",
    "    def SGD_OLS(self, n_datapoints, batch_size, n_epochs, scaling_algorithm = ScalingAlgorithm.StochasticGD, rho = None, beta_values = None, momentum = None):\n",
    "        m = int(n_datapoints/batch_size)\n",
    "        beta = np.random.randn(3,1)\n",
    "\n",
    "        betas, scores = list(), list()\n",
    "        for epoch in range(n_epochs):\n",
    "            beta = self._SGD(beta, LinearRegressionModel.OrdinaryLeastSquares, epoch, batch_size, m, scaling_algorithm, rho=rho, beta_values=beta_values, momentum=momentum)\n",
    "            \n",
    "            #Store results\n",
    "            betas.append(beta)\n",
    "            scores.append(MSE(self.y, self.design_matrix @ beta))\n",
    "        \n",
    "        return [betas, scores]\n",
    "    \n",
    "    def SGD_Ridge(self, n_datapoints, batch_size, n_epochs, lmbda, scaling_algorithm = ScalingAlgorithm.StochasticGD):\n",
    "        m = int(n_datapoints/batch_size)\n",
    "        beta = np.random.randn(3,1)\n",
    "\n",
    "        betas, scores = list(), list()\n",
    "        for epoch in range(n_epochs):\n",
    "            beta = self._SGD(beta, LinearRegressionModel.Ridge, epoch, batch_size, m, scaling_algorithm, lmbda)\n",
    "\n",
    "            #Store results\n",
    "            betas.append(beta)\n",
    "            scores.append(MSE(self.y, self.design_matrix @ beta))\n",
    "\n",
    "        return [betas, scores]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_best_mse_and_eta(mse_data, learning_rates):\n",
    "    min_mse = np.min(mse_data)\n",
    "    best_index = np.argmin(mse_data)\n",
    "    best_learning_rate = learning_rates[best_index]\n",
    "\n",
    "    return [min_mse, best_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set-up taken from lecture notes\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+5*x*x + np.random.randn(n, 1)\n",
    "\n",
    "N_iterations = 30\n",
    "\n",
    "linear_regression = LinearRegression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent\n",
    "learning_rates = np.array([0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1])\n",
    "mse_data = np.zeros((len(learning_rates), 2))  # Rows for learning rates, columns for OLS and Ridge\n",
    "mse_data_test = list()\n",
    "for i, eta in enumerate(learning_rates):\n",
    "    ols_betas, ols_mses = linear_regression.gradient_descent_OLS(n, N_iterations, eta)\n",
    "    mse_data[i, 0] = min(ols_mses)\n",
    "    ridge_betas, ridge_mses = linear_regression.gradient_descent_ridge(n, N_iterations, eta, lmbda=0.01)\n",
    "    mse_data[i, 1] = min(ridge_mses)\n",
    "\n",
    "    # print(f\"OLS - eta: {eta}, mse: {ols_mse}\")\n",
    "    # print(f\"Ridge - eta: {eta}, mse: {ridge_mse}\")\n",
    "\n",
    "\n",
    "# Find the minimum MSE and corresponding index\n",
    "ols_min_mse, ols_best_eta = Get_best_mse_and_eta(mse_data[:, 0], learning_rates)\n",
    "ridge_min_mse, ridge_best_eta = Get_best_mse_and_eta(mse_data[:, 1], learning_rates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best learning rate for OLS: {ols_best_eta} with MSE: {ols_min_mse}\")\n",
    "print(f\"Best learning rate for Ridge: {ridge_best_eta} with MSE: {ridge_min_mse}\")\n",
    "\n",
    "# Get values for the best learning rate: (I'm lazy and run it again rather than rewrite the loop above to store it)\n",
    "ols_betas, ols_mses = linear_regression.gradient_descent_OLS(n, N_iterations, ols_best_eta)\n",
    "ridge_betas, ridge_mses = linear_regression.gradient_descent_ridge(n, N_iterations, ridge_best_eta, lmbda=0.01)\n",
    "\n",
    "\n",
    "# Plot MSE over iterations\n",
    "plt.plot(range(N_iterations), ols_mses, label=\"MSE (OLS) over iterations\")\n",
    "plt.plot(range(N_iterations), ridge_mses, label=\"MSE (Ridge) over iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Gradient Descent Optimization of MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a DataFrame for heatmap plotting\n",
    "mse_df = pd.DataFrame(mse_data, columns=['OLS MSE', 'Ridge MSE'], index=learning_rates)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "#sns.heatmap(mse_df, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'MSE'})\n",
    "sns.heatmap(mse_df, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar=False)\n",
    "plt.title(\"OLS and Ridge Regression Across Learning Rates\")\n",
    "plt.xlabel(\"Regression Type\")\n",
    "plt.ylabel(\"Learning Rates\")\n",
    "plt.xticks(ticks=np.arange(0.5, 2), labels=mse_df.columns, rotation=0)\n",
    "plt.yticks(ticks=np.arange(len(learning_rates)), labels=mse_df.index, rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Gradient descent with momentum\n",
    "learning_rates = np.array([0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1])\n",
    "mse_data = np.zeros((len(learning_rates), 2))  # Rows for learning rates, columns for OLS and Ridge\n",
    "for i, eta in enumerate(learning_rates):\n",
    "    #OLS\n",
    "    ols_betas, ols_mses = linear_regression.gradient_descent_OLS(n, N_iterations, eta, ScalingAlgorithm.MomentumGD, momentum=momentum)\n",
    "    mse_data[i, 0] = min(ols_mses)\n",
    "    ridge_betas, ridge_mses = linear_regression.gradient_descent_ridge(n, N_iterations, eta, 0.01, momentum=momentum)\n",
    "    mse_data[i, 1] = min(ridge_mses)\n",
    "\n",
    "    #print(f\"OLS - eta: {eta}, mse: {ols_mse}    Ridge - eta: {eta}, mse: {ridge_mse}\")\n",
    "\n",
    "# Find the minimum MSE and corresponding index\n",
    "ols_min_mse, ols_best_eta = Get_best_mse_and_eta(mse_data[:, 0], learning_rates)\n",
    "ridge_min_mse, ridge_best_eta = Get_best_mse_and_eta(mse_data[:, 1], learning_rates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best learning rate for OLS: {ols_best_eta} with MSE: {ols_min_mse}\")\n",
    "print(f\"Best learning rate for Ridge: {ridge_best_eta} with MSE: {ridge_min_mse}\")\n",
    "\n",
    "# Get values for the best learning rate: (I am lazy and run it again rather than rewrite the loop above to store it)\n",
    "ols_betas, ols_mses = linear_regression.gradient_descent_OLS(n, N_iterations, ols_best_eta, ScalingAlgorithm.MomentumGD, momentum=momentum)\n",
    "ridge_betas, ridge_mses = linear_regression.gradient_descent_ridge(n, N_iterations, ridge_best_eta, 0.01, momentum=momentum)\n",
    "\n",
    "\n",
    "# Plot MSE over iterations\n",
    "plt.plot(range(N_iterations), ols_mses, label=\"MSE (OLS) over iterations\")\n",
    "plt.plot(range(N_iterations), ridge_mses, label=\"MSE (Ridge) over iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Gradient Descent Optimization of MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for heatmap plotting\n",
    "mse_df = pd.DataFrame(mse_data, columns=['OLS MSE', 'Ridge MSE'], index=learning_rates)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "#sns.heatmap(mse_df, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'MSE'})\n",
    "sns.heatmap(mse_df, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar=False)\n",
    "plt.title(\"OLS and Ridge Regression Across Learning Rates w/ Memory\")\n",
    "plt.xlabel(\"Regression Type\")\n",
    "plt.ylabel(\"Learning Rates\")\n",
    "plt.xticks(ticks=np.arange(0.5, 2), labels=mse_df.columns, rotation=0)\n",
    "plt.yticks(ticks=np.arange(len(learning_rates)), labels=mse_df.index, rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD with batches and epocs\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Test for different nr epochs\n",
    "epochs = np.array([1, 10, 50, 100, 500, 1000])\n",
    "#.. and batch size\n",
    "batch_sizes = np.array([1, 5, 10, 20, 50])\n",
    "mse_data_ols = np.zeros((len(batch_sizes), len(epochs)))  # Arrays for epochs and batch sizes\n",
    "mse_data_ridge = np.zeros((len(batch_sizes), len(epochs)))  # Arrays for epochs and batch sizes\n",
    "for j, M in enumerate(batch_sizes):\n",
    "    for i, n_epochs in enumerate(epochs):\n",
    "        beta_ols, scores_ols = linear_regression.SGD_OLS(n, M, n_epochs)\n",
    "        mse_data_ols[j, i] = min(scores_ols)\n",
    "\n",
    "        beta_ridge, scores_ridge = linear_regression.SGD_Ridge(n, M, n_epochs, lmbda=0.01)\n",
    "        mse_data_ridge[j, i] = min(scores_ridge)\n",
    "\n",
    "# Find the minimum MSE and corresponding indices for OLS\n",
    "ols_min_mse_index = np.unravel_index(np.argmin(mse_data_ols, axis=None), mse_data_ols.shape)\n",
    "ols_min_mse = mse_data_ols[ols_min_mse_index]\n",
    "ols_best_mini_batch_size = batch_sizes[ols_min_mse_index[0]]\n",
    "ols_best_n_epochs = epochs[ols_min_mse_index[1]]\n",
    "\n",
    "# Find the minimum MSE and corresponding indices for Ridge\n",
    "ridge_min_mse_index = np.unravel_index(np.argmin(mse_data_ridge, axis=None), mse_data_ridge.shape)\n",
    "ridge_min_mse = mse_data_ridge[ridge_min_mse_index]\n",
    "ridge_best_mini_batch_size = batch_sizes[ridge_min_mse_index[0]]\n",
    "ridge_best_n_epochs = epochs[ridge_min_mse_index[1]]\n",
    "\n",
    "# Print results\n",
    "print(f\"Best settings for OLS: Mini-batch size = {ols_best_mini_batch_size}, Epochs = {ols_best_n_epochs} with MSE: {ols_min_mse}\")\n",
    "print(f\"Best settings for Ridge: Mini-batch size = {ridge_best_mini_batch_size}, Epochs = {ridge_best_n_epochs} with MSE: {ridge_min_mse}\")\n",
    "\n",
    "# Create a DataFrame for heatmap plotting\n",
    "mse_df_ols = pd.DataFrame(mse_data_ols, index=batch_sizes, columns=epochs)\n",
    "mse_df_ridge = pd.DataFrame(mse_data_ridge, index=batch_sizes, columns=epochs)\n",
    "\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# OLS MSE Heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(mse_df_ols, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'MSE'})\n",
    "plt.title(\"OLS MSE Across Epochs and Mini-batch Sizes\")\n",
    "plt.xlabel(\"Nr of Epochs\")\n",
    "plt.ylabel(\"Mini-batch Size\")\n",
    "\n",
    "# Ridge MSE Heatmap\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(mse_df_ridge, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'MSE'})\n",
    "plt.title(\"Ridge MSE Across Epochs and Mini-batch Sizes\")\n",
    "plt.xlabel(\"Nr of Epochs\")\n",
    "plt.ylabel(\"Mini-batch Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adagrad\n",
    "n = 100\n",
    "n_iterations = 1000\n",
    "eta = 0.1\n",
    "momentum = 0.3\n",
    "batch_size = 5\n",
    "epochs = 100\n",
    "\n",
    "#Regular adagrad\n",
    "ada_betas, ada_mses = linear_regression.gradient_descent_OLS(n, n_iterations, eta, ScalingAlgorithm.Adagrad)\n",
    "print(f\"Best GD AdaGrad: {min(ada_mses)}\")\n",
    "\n",
    "#Adagrad with momentum\n",
    "ada_betas, ada_mses = linear_regression.gradient_descent_OLS(n, n_iterations, eta, ScalingAlgorithm.Adagrad, momentum)\n",
    "print(f\"Best GD AdaGrad with momentum: {min(ada_mses)}\")\n",
    "\n",
    "#SGD with AdaGrad\n",
    "ada_betas, ada_mses = linear_regression.SGD_OLS(n, batch_size, epochs, ScalingAlgorithm.Adagrad)\n",
    "print(f\"Best SGD AdaGrad: {min(ada_mses)}\")\n",
    "\n",
    "#SGD with AdaGrad and momentum\n",
    "ada_betas, ada_mses = linear_regression.SGD_OLS(n, batch_size, epochs, ScalingAlgorithm.Adagrad, momentum=momentum)\n",
    "print(f\"Best SGD AdaGrad with momentum: {min(ada_mses)}\")\n",
    "\n",
    "#For comparison\n",
    "#GD:\n",
    "gd_betas, gd_mses = linear_regression.gradient_descent_OLS(n, n_iterations, eta, ScalingAlgorithm.GradientDescent)\n",
    "print(f\"Best GD: {min(gd_mses)}\")\n",
    "\n",
    "#GD with momentum:\n",
    "gd_betas, gd_mses = linear_regression.gradient_descent_OLS(n, n_iterations, eta, ScalingAlgorithm.GradientDescent, momentum=momentum)\n",
    "print(f\"Best GD with momentum: {min(gd_mses)}\")\n",
    "\n",
    "#SGD:\n",
    "sgd_betas, sgd_mses = linear_regression.SGD_OLS(n, batch_size, epochs)\n",
    "print(f\"Best SGD: {min(sgd_mses)}\")\n",
    "\n",
    "#SGD with momentum:\n",
    "sgd_betas, sgd_mses = linear_regression.SGD_OLS(n, batch_size, epochs, momentum=momentum)\n",
    "print(f\"Best SGD with momentum: {min(sgd_mses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop\n",
    "betas, rms_prop_mses = linear_regression.gradient_descent_OLS(n=100, n_iterations=1000, eta=0.1, scaling_algorithm=ScalingAlgorithm.RMSprop, rho=0.9)\n",
    "print(f\"Best GD RMSprop: {min(rms_prop_mses)}\")\n",
    "\n",
    "#RMSprop with momentum\n",
    "betas, rms_prop_mses = linear_regression.gradient_descent_OLS(n=100, n_iterations=1000, eta=0.1, scaling_algorithm=ScalingAlgorithm.RMSprop, rho=0.9, momentum=0.9)\n",
    "print(f\"Best GD RMSprop with momentum: {min(rms_prop_mses)}\")\n",
    "\n",
    "#SGD with RMSprop\n",
    "betas, rms_prop_mses = linear_regression.SGD_OLS(n_datapoints=100, batch_size=5, n_epochs=100, scaling_algorithm=ScalingAlgorithm.RMSprop, rho=0.9)\n",
    "print(f\"Best SGD RMSprop: {min(rms_prop_mses)}\")\n",
    "\n",
    "#SGD with RMSprop with momentum\n",
    "betas, rms_prop_mses = linear_regression.SGD_OLS(n_datapoints=100, batch_size=5, n_epochs=100, scaling_algorithm=ScalingAlgorithm.RMSprop, rho=0.9, momentum=0.9)\n",
    "print(f\"Best SGD RMSprop with momentum: {min(rms_prop_mses)}\")\n",
    "\n",
    "#SGD with ADAM\n",
    "betas, rms_prop_mses = linear_regression.SGD_OLS(n_datapoints=100, batch_size=5, n_epochs=100, scaling_algorithm=ScalingAlgorithm.Adam, beta_values=[0.9, 0.999])\n",
    "print(f\"Best SGD ADAM: {min(rms_prop_mses)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) Writing your own neural network code (Central part of the project)\n",
    "Implement back propagation algorithm discussed in lectures week 41 and 42\n",
    "### Regression problem first: Can use simple 2nd order poly from part a, Franke or terrain. Discuss choice of cost fcn\n",
    "- Decide on cost fcn\n",
    "- With Sigmoid as activation fcn for hidden layers, and a flexible nr of hidden layers, write an FFNN\n",
    "- Initialise wheights using a normal distribution\n",
    "- How would you initialise the biases?\n",
    "- Which activation would you choose for the final output layer?\n",
    "- Train your network\n",
    "\n",
    "If running on Franke:\n",
    "- compare your results from OLS and Ridge from project 1 (if using franke or terrain)\n",
    "\n",
    "But also:\n",
    "- Compare to similar code in scikit\n",
    "Comment on results and compare linear regression to this FFNN\n",
    "Make an analysis of the regularisation parameters and the learning rates employed to find the optimal MSE and R2 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FFNN\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "\n",
    "class ActivationFunction(Enum):\n",
    "    Sigmoid = 0,\n",
    "    RELU = 1,\n",
    "    LeakyRELU = 2\n",
    "\n",
    "class CostFunction(Enum):\n",
    "    MSE = 0,\n",
    "    R2 = 1\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z)) \n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, \n",
    "                 #output_layer, \n",
    "                 cost_function, \n",
    "                 #Output layer:\n",
    "                 outputlayer_activation = None, \n",
    "                 n_output_neurons = 1, \n",
    "                 n_outputs = 1, \n",
    "                 #Hidden layer:\n",
    "                 hidden_activation = ActivationFunction.Sigmoid,\n",
    "                 hidden_features = 1,\n",
    "                 hidden_neurons = 1,\n",
    "                 hidden_layers = None):\n",
    "        #self.input_layer = input_layer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.cost_function = cost_function\n",
    "        output_layer = OutputLayer(outputlayer_activation, n_outputs)\n",
    "\n",
    "        #User can self-define a list of hidden layers, otherwise the default is one layer with given values\n",
    "        if hidden_layers == None:\n",
    "            self.hidden_layers = list([HiddenLayer(hidden_activation, hidden_features, hidden_neurons)])\n",
    "        \n",
    "        n_neurons_last_hiddenlayer = self.hidden_layers[-1]._get_hidden_neurons()\n",
    "        output_layer._set_weights(n_neurons_last_hiddenlayer)\n",
    "        self.output_layer = output_layer\n",
    "            \n",
    "    \n",
    "    #So as to be flexible, we can expand the model after creation - will probably need a method like GetHiddenLayers\n",
    "    def add_hidden_layer(self, position, layer):\n",
    "        self.hidden_layers.append(layer) #TODO: At a specific position\n",
    "            \n",
    "    def evaluate_model(self, predicted_value, target_value):\n",
    "        match self.cost_function:\n",
    "            case CostFunction.MSE:\n",
    "                return np.mean((predicted_value - target_value) ** 2)\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"Cost function {self.cost_function} not implemented.\")\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        activations = []\n",
    "        a = x  # Initial input is x\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            z = np.matmul(a, layer.get_weights()) + layer.get_biases()\n",
    "            a = layer.activation(z)\n",
    "            activations.append(a)  # Collect activation from each layer for backprop\n",
    "        # Output layer computation (no activation here for regression)\n",
    "        z_out = np.matmul(a, self.output_layer.get_weights()) + self.output_layer.get_biases()\n",
    "        activations.append(z_out)\n",
    "        return activations\n",
    "    \n",
    "    def BackPropagation(self, activations, y, x):\n",
    "        # Initializing gradients\n",
    "        output_weights_gradient = []\n",
    "        output_bias_gradient = []\n",
    "        \n",
    "        hidden_weights_gradient = []\n",
    "        hidden_bias_gradient = []\n",
    "        \n",
    "        a_output = activations[-1]\n",
    "        delta_output = (a_output - y) * self.output_layer.derivative(a_output)\n",
    "\n",
    "        #print(0.5 * ((a_output - y) ** 2))  # MSE for linear output\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        delta = np.matmul(delta_output, self.output_layer.get_weights().T) * self.hidden_layers[-1].derivative(activations[-2])\n",
    "        output_weights_gradient = np.matmul(activations[-2].T, delta_output)\n",
    "        output_bias_gradient = np.sum(delta_output, axis=0, keepdims=True)\n",
    "\n",
    "        for i in reversed(range(len(self.hidden_layers))):\n",
    "            # Insert at start of list because working backwards\n",
    "            hidden_weights_gradient.insert(0, np.matmul(activations[i-1].T, delta) if i > 0 else np.matmul(x.T, delta))\n",
    "            hidden_bias_gradient.insert(0, np.sum(delta, axis=0, keepdims=True))\n",
    "\n",
    "            # Prepare delta for the next layer back\n",
    "            if i > 0:\n",
    "                delta = np.matmul(delta, self.hidden_layers[i].get_weights().T) * self.hidden_layers[i-1].derivative(activations[i-1])\n",
    "\n",
    "        \n",
    "        return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "    \n",
    "    # Update weights and biases using gradients\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient = gradients\n",
    "        \n",
    "        # Update output layer weights and biases\n",
    "        self.output_layer.w -= learning_rate * output_weights_gradient\n",
    "        self.output_layer.b -= learning_rate * output_bias_gradient.squeeze()\n",
    "        \n",
    "        # Update hidden layer weights and biases\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            layer.w -= learning_rate * hidden_weights_gradient[i]\n",
    "            layer.b -= learning_rate * hidden_bias_gradient[i].squeeze()\n",
    "\n",
    "    \n",
    "    # Train the model\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        final_loss = 0\n",
    "        #Becomes relevant if adding a stop condition:\n",
    "        final_epoch = 0\n",
    "        for epoch in range(epochs):\n",
    "            final_epoch = epoch + 1\n",
    "            # Feedforward pass\n",
    "            activations = self.FeedForward(x)\n",
    "            # Compute loss for monitoring (optional)\n",
    "            loss = self.evaluate_model(activations[-1], y)\n",
    "            final_loss = loss\n",
    "            #print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            # Backpropagation\n",
    "            gradients = self.BackPropagation(activations, y, x)\n",
    "            # Update weights and biases\n",
    "            self.update_parameters(gradients, learning_rate)\n",
    "\n",
    "        return [final_epoch, final_loss]\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, activation_function, n_features, n_hidden_neurons):\n",
    "        # weights and bias in the hidden layer\n",
    "        self.w = np.random.randn(n_features, n_hidden_neurons)\n",
    "        #self.b = np.zeros(n_hidden_neurons) #+ 0.01 \n",
    "        self.b = np.zeros((1, n_hidden_neurons))\n",
    "        #TODO: For different activation functions, a small initial value may be beneficial, especially for relu to avoid vanishing neurons\n",
    "        self.activation_fnc = activation_function\n",
    "        self.n_neurons = n_hidden_neurons\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return self.b\n",
    "    \n",
    "    def activation(self, z):\n",
    "        match self.activation_fnc:\n",
    "            case ActivationFunction.Sigmoid:\n",
    "                return sigmoid(z)\n",
    "            case None:\n",
    "                return z # Typical for regression type problems\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"Activation function not implemented for: {self.activation_fnc}\")\n",
    "\n",
    "    def derivative(self, a):\n",
    "        match self.activation_fnc:\n",
    "            case ActivationFunction.Sigmoid:\n",
    "                return sigmoid_derivative(a)\n",
    "            case None:\n",
    "                return np.ones_like(a) #Linear #TODO: Is this right?\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"Derivative function not implemented for {self.activation_fnc}\")\n",
    "\n",
    "class HiddenLayer(Layer):\n",
    "    pass\n",
    "    def _get_hidden_neurons(self):\n",
    "        return self.n_neurons\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, activation_function, n_outputs):\n",
    "        # weights and bias in the output layer\n",
    "        #self.b = np.zeros(n_outputs) #+ 0.01 \n",
    "        self.n_outputs = n_outputs\n",
    "        self.b = np.zeros((1, n_outputs))\n",
    "        self.activation_fnc = activation_function\n",
    "\n",
    "    def _set_weights(self, n_hidden_neurons):\n",
    "        self.w = np.random.randn(n_hidden_neurons, self.n_outputs)\n",
    "\n",
    "#TODO: Input layer if/when needed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 1.1280593056418667e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We use the Sigmoid function as activation function\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "# Input variable\n",
    "x = np.array([4.0],dtype=np.float64)\n",
    "# Target values\n",
    "y = 2*x+1.0 \n",
    "y += np.random.normal(0, 100, y.shape)\n",
    "\n",
    "# Defining the neural network, only scalars here\n",
    "n_inputs = x.shape\n",
    "n_features = 1\n",
    "n_hidden_neurons = 1\n",
    "n_outputs = 1\n",
    "\n",
    "n_iterations = 50\n",
    "#TODO: Some stopping condition\n",
    "\n",
    "#Set up and train simple model\n",
    "model = NeuralNetwork(CostFunction.MSE)\n",
    "epochs, loss = model.train(x, y, n_iterations, learning_rate=0.1)\n",
    "\n",
    "\n",
    "print(f\"Epoch {epochs}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with Scikit-learn: 1953.0277026047493\n",
      "Mean Squared Error with custom neural net: 236.97401363469467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Compare to scikit learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Generate synthetic data for regression\n",
    "# Ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
    "\n",
    "# 2. Initialize and fit the Scikit-learn MLPRegressor\n",
    "scikit_model = MLPRegressor(hidden_layer_sizes=(1,),  # You can change the size and number of hidden layers\n",
    "                             learning_rate_init=0.001,\n",
    "                             max_iter=50,\n",
    "                             random_state=0)\n",
    "\n",
    "scikit_model.fit(X, y)\n",
    "\n",
    "# 3. Predictions and evaluation using Scikit-learn\n",
    "y_pred_sklearn = scikit_model.predict(X)\n",
    "mse_sklearn = mean_squared_error(y, y_pred_sklearn)\n",
    "print(f\"Mean Squared Error with Scikit-learn: {mse_sklearn}\")\n",
    "\n",
    "# 4. Initialize and train custom neural network\n",
    "hidden_layers = list([HiddenLayer(ActivationFunction.Sigmoid, n_features=1, n_hidden_neurons=5)])\n",
    "custom_model = NeuralNetwork(CostFunction.MSE, hidden_layers=hidden_layers)\n",
    "\n",
    "epoch, loss = custom_model.train(X, y.reshape(-1, 1), epochs=50, learning_rate=0.001)  # Reshape y to match expected input\n",
    "print(f\"Mean Squared Error with custom neural net: {loss}\")\n",
    "\n",
    "# iterations = 300\n",
    "# for iter in iterations:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
