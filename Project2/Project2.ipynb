{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression, from linear and logistic regression to neural networks\n",
    "\n",
    "## Regression:\n",
    "Bring back regression code from project 1 (reference own work in bibliography) and compare to results from FFNN\n",
    "Data can be:\n",
    "- simple 1d\n",
    "- Franke\n",
    "- anything\n",
    "But start with something simple\n",
    "\n",
    "## Classification:\n",
    "Develop a logistic regression code and compare to FFNN.\n",
    "Proposed data set:\n",
    "- Wisconsin Breast Cancer Data (images representing various tumor features)\n",
    "- Andre data (men pass p√• at kan diskutere resultatene mot literature)\n",
    "\n",
    "\n",
    "## Part a) Write own Stochastic Gradient Descent Code: First step\n",
    "Replace matrix inversion from OLS and Ridge with GD and SGD\n",
    "Data can be either\n",
    "- Franke\n",
    "- Terrain \n",
    "- Recommended: something simple like for instance: f(x) = ao + a1x + a2x^2\n",
    "look at exercises from week 41 for help\n",
    "\n",
    "## Analyse the GD and SGD code. This should include:\n",
    "(Lecture notes week 39 and 40 contain examples it is ok to use)\n",
    "### 1: plain gradient descent with fixed learning rate (tune it) using the analytical expression for the gradient\n",
    "### 2: Add momentum to the plain GD code and compare convergence with a fixed learning rate (may need to tune it). Keep using the analytical expression for the gradient\n",
    "### 3: Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. Use a tunable learning rate (lectures week 39 and 40). Discuss the results as functions of the various parameters (size of batches, nr of epochs etc). Use the analytical gradient\n",
    "### 4: Implement the adagrad method in order to tune the learning rate. Do this with and without momentum for plain gradient descent and SGD\n",
    "### 5: Add RMSprop and Adam to your library of methods for tuning the learning rate\n",
    "### 6: Replace thereafter your analytical gradient with either Autograd or JAX\n",
    "\n",
    "## Part b) Writing your own neural network code (Central part of the project)\n",
    "Implement back propagation algorithm discussed in lectures week 41 and 42\n",
    "### Regression problem first: Can use simple 2nd order poly from part a, Franke or terrain. Discuss choice of cost fcn\n",
    "Write a FFNN with a flexible nr of hidden layers and nodes using the sigmoid fcn as activation fcn for the hidden layers. Initialise the weights using a normal distribution. \n",
    "How would you initialise the biases?\n",
    "Which activation would yuu choose for the final output layer?\n",
    "Train your network and compare your results from OLS and Ridge from project 1 (if using franke or terrain)\n",
    "Compare to similar code in schikit\n",
    "Comment on results and compare linear regression to this FFNN\n",
    "Make an analysis of the regularisation parameters and the learning rates employed to find the optimal MSE and R2 scores.\n",
    "\n",
    "## Part c) Testing different activation functions for the hidden layers\n",
    "- Sigmoid\n",
    "- RELU\n",
    "- Leaky RELU\n",
    "Discuss the results\n",
    "May also study the way you initialise the  weights and biases\n",
    "\n",
    "## Part d) Classification analysis using neural networks\n",
    "\n",
    "## Part e) Write your logistic regression code\n",
    "To compare the FFNN code with logistic regression\n",
    "\n",
    "## Part f) Critical evaluation of the various algorithms\n",
    "Compare the results from the different algorithms, which works best for the regression case, and which one for classification? List pros and cons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "\n",
    "def MSE(expected_values, predicted_values):\n",
    "    n = np.size(predicted_values)  # Number of data points\n",
    "    return np.sum((expected_values - predicted_values)**2)/n\n",
    "\n",
    "class ScalingAlgorithm(Enum):\n",
    "    GradientDescent = 0,\n",
    "    StochasticGD = 1,\n",
    "    Adagrad = 2,\n",
    "    RMSProp = 3,\n",
    "    Adam = 4\n",
    "\n",
    "class LinearRegressionModel(Enum):\n",
    "    OrdinaryLeastSquares = 0,\n",
    "    Ridge = 1\n",
    "\n",
    "class OLS_Hessian:\n",
    "    def __init__(self, n, X):\n",
    "        self.H = (2.0/n)* X.T @ X\n",
    "        self.EigValues, self.EigVectors = np.linalg.eig(self.H)\n",
    "\n",
    "class Ridge_Hessian:\n",
    "    def __init__(self, n, lmbda, XT_X):\n",
    "        self.H = (2.0/n)* XT_X+2*lmbda* np.eye(XT_X.shape[0])\n",
    "        self.EigValues, self.EigVectors = np.linalg.eig(self.H)\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, x, y):\n",
    "        # Design matrix including the intercept\n",
    "        # No scaling of data and all data used for training \n",
    "        n = x.shape[0]\n",
    "        self.design_matrix = np.c_[np.ones((n, 1)), x, x*x]\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        self.ols_hessian = None\n",
    "        self.ridge_hessian = None\n",
    "\n",
    "    #Function contents taken from lecture notes\n",
    "    def _gradient_descent_OLS(self, n, n_iterations, eta):\n",
    "        #Don't wanna calculate this every time when running multiple iterations\n",
    "        if not self.ols_hessian:\n",
    "            self.ols_hessian = OLS_Hessian(n, self.design_matrix)\n",
    "        \n",
    "        beta = np.random.randn(3,1)\n",
    "        #TODO: Look at how to tune learning rates\n",
    "        eta = 1.0/np.max(self.ols_hessian.EigValues)\n",
    "        print(\"Note: Overwriting input-eta to scale based on eigenvalue\")\n",
    "        for iter in range(n_iterations):\n",
    "            gradient = (2.0/n)*self.design_matrix.T @ (self.design_matrix @ beta-self.y)\n",
    "            beta -= eta*gradient\n",
    "\n",
    "        return beta\n",
    "    \n",
    "    #Function contents taken from lecture notes\n",
    "    def _gradient_descent_ridge(self, n, n_iterations, eta, lmbda): \n",
    "        #Don't wanna calculate this every time when running multiple iterations\n",
    "        if not self.ridge_hessian:\n",
    "            XT_X = self.design_matrix.T @ self.design_matrix\n",
    "            self.ridge_hessian = Ridge_Hessian(n, lmbda, XT_X)\n",
    "        \n",
    "        #Id = n*lmbda* np.eye((XT_X).shape[0])\n",
    "\n",
    "        # Gradient descent with Ridge\n",
    "        beta = np.random.randn(3,1)\n",
    "        eta = 1.0/np.max(self.ridge_hessian.EigValues)\n",
    "        #TODO: Tuning learning rate\n",
    "        print(\"Note: Overwriting input-eta to scale based on eigenvalue\")\n",
    "        for iter in range(n_iterations):\n",
    "            gradient = 2.0/n*self.design_matrix.T @ (self.design_matrix @ beta-self.y)+2*lmbda*beta\n",
    "            beta -= eta*gradient\n",
    "\n",
    "        return beta\n",
    "    \n",
    "    def gradient_descent(self, lin_reg_model, n_datapoints, n_iterations, learning_rate, Lambda = None, nr_minibatches = None, epochs = None, scaling_algorithm = None):\n",
    "        if nr_minibatches or epochs or scaling_algorithm:\n",
    "            return NotImplementedError\n",
    "        \n",
    "        match lin_reg_model:\n",
    "            case LinearRegressionModel.OrdinaryLeastSquares:\n",
    "                beta = self._gradient_descent_OLS(n_datapoints, n_iterations, learning_rate)\n",
    "            \n",
    "            case LinearRegressionModel.Ridge:\n",
    "                if not Lambda:\n",
    "                    raise ValueError(\"Lambda value cannot be null\")\n",
    "                beta = self._gradient_descent_ridge(n_datapoints, n_iterations, learning_rate, Lambda)\n",
    "                \n",
    "        y_tilde = self.design_matrix @ beta\n",
    "        mse = MSE(self.y, y_tilde)\n",
    "        \n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[10.60049665  0.67439909  0.03065854]\n",
      "Note: Overwriting input-eta to scale based on eigenvalue\n",
      "Eigenvalues of Hessian Matrix:[10.62049665  0.69439909  0.05065854]\n",
      "Note: Overwriting input-eta to scale based on eigenvalue\n",
      "MSE from gradient descent OLS: 0.001104370464615587\n",
      "MSE from gradient descent Ridge: 0.02133001729571076\n"
     ]
    }
   ],
   "source": [
    "#Set-up taken from lecture notes\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+5*x*x\n",
    "# Learning rate and number of iterations\n",
    "eta = 0.05\n",
    "N_iterations = 100\n",
    "\n",
    "linear_regression = LinearRegression(x, y)\n",
    "\n",
    "#Gradient descent\n",
    "##OLS\n",
    "ols_mse = linear_regression.gradient_descent(LinearRegressionModel.OrdinaryLeastSquares, n, N_iterations, eta)\n",
    "##Ridge\n",
    "ridge_mse = linear_regression.gradient_descent(LinearRegressionModel.Ridge, n, N_iterations, eta, Lambda=0.01)\n",
    "\n",
    "print(f\"MSE from gradient descent OLS: {ols_mse}\")\n",
    "print(f\"MSE from gradient descent Ridge: {ridge_mse}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
